{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Ingestion and 2. Data Processing:"
      ],
      "metadata": {
        "id": "MNVxTsUy6A3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install avro-python3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KryA9w91mROC",
        "outputId": "dc06bfb6-da41-4a42-e790-3ef07c89541f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: avro-python3 in /usr/local/lib/python3.10/dist-packages (1.10.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUVLrrW0l4Wj",
        "outputId": "5982e783-b3d2-4246-94ae-7d568552f651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingesting ad impression: {'ad_creative_id': 1, 'user_id': '101', 'timestamp': '2024-05-02T10:00:00', 'website': 'example.com'}\n",
            "Ingesting ad impression: {'ad_creative_id': 2, 'user_id': '102', 'timestamp': '2024-05-02T11:00:00', 'website': 'example.net'}\n",
            "Ingesting click/conversion: {'user_id': 'HOSIV', 'timestamp': '2024-05-02 18:16:53.786989', 'ad_campaign_id': 5, 'conversion_type': 'download'}\n",
            "Ingesting click/conversion: {'user_id': 'VS50G', 'timestamp': '2024-05-02 18:19:53.787059', 'ad_campaign_id': 2, 'conversion_type': 'download'}\n",
            "Ingesting click/conversion: {'user_id': 'JWW2S', 'timestamp': '2024-05-02 17:55:53.787071', 'ad_campaign_id': 1, 'conversion_type': 'sign-up'}\n",
            "Ingesting click/conversion: {'user_id': '1KGLL', 'timestamp': '2024-05-02 18:27:53.787081', 'ad_campaign_id': 4, 'conversion_type': 'sign-up'}\n",
            "Ingesting click/conversion: {'user_id': '6P7MH', 'timestamp': '2024-05-02 18:06:53.787090', 'ad_campaign_id': 3, 'conversion_type': 'purchase'}\n",
            "Ingesting click/conversion: {'user_id': 'EOBHB', 'timestamp': '2024-05-02 18:01:53.787100', 'ad_campaign_id': 1, 'conversion_type': 'download'}\n",
            "Ingesting click/conversion: {'user_id': 'ZVDBV', 'timestamp': '2024-05-02 17:52:53.787122', 'ad_campaign_id': 4, 'conversion_type': 'download'}\n",
            "Ingesting click/conversion: {'user_id': 'JUHGB', 'timestamp': '2024-05-02 18:01:53.787131', 'ad_campaign_id': 4, 'conversion_type': 'download'}\n",
            "Ingesting click/conversion: {'user_id': 'NQ6JO', 'timestamp': '2024-05-02 17:51:53.787139', 'ad_campaign_id': 1, 'conversion_type': 'sign-up'}\n",
            "Ingesting click/conversion: {'user_id': 'H2K0J', 'timestamp': '2024-05-02 18:25:53.787148', 'ad_campaign_id': 2, 'conversion_type': 'download'}\n",
            "Ingesting bid request: {'user_id': 'T0JS0', 'auction_id': 'N6OWCPUL84', 'ad_targeting_criteria': {'age': 64, 'gender': 'female', 'location': 'USA'}}\n",
            "Ingesting bid request: {'user_id': '6IO16', 'auction_id': 'T9NABDKDB9', 'ad_targeting_criteria': {'age': 58, 'gender': 'male', 'location': 'Australia'}}\n",
            "Ingesting bid request: {'user_id': '38D4W', 'auction_id': '7PB9YO03AJ', 'ad_targeting_criteria': {'age': 65, 'gender': 'male', 'location': 'Canada'}}\n",
            "Correlated data: [{'impression': {'ad_creative_id': 1, 'user_id': '101', 'timestamp': '2024-05-02T10:00:00', 'website': 'example.com'}, 'click_conversion': {'user_id': '101', 'timestamp': '2024-05-02T10:05:00', 'ad_campaign_id': 1, 'conversion_type': 'sign-up'}}, {'impression': {'ad_creative_id': 2, 'user_id': '102', 'timestamp': '2024-05-02T11:00:00', 'website': 'example.net'}, 'click_conversion': {'user_id': '102', 'timestamp': '2024-05-02T11:15:00', 'ad_campaign_id': 2, 'conversion_type': 'purchase'}}]\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import string\n",
        "import datetime\n",
        "from avro.io import DatumWriter\n",
        "from avro.datafile import DataFileWriter\n",
        "\n",
        "# Define the DataIngester class\n",
        "class DataIngester:\n",
        "    def __init__(self):\n",
        "        # Initialize any necessary resources or connections here\n",
        "        pass\n",
        "\n",
        "    def ingest_ad_impressions(self, json_data):\n",
        "        # Process ad impressions JSON data\n",
        "        for impression in json_data:\n",
        "            # Perform ingestion operations (e.g., store in database, queue for further processing)\n",
        "            print(\"Ingesting ad impression:\", impression)\n",
        "\n",
        "    def ingest_clicks_conversions(self, data):\n",
        "        # Process clicks/conversions data\n",
        "        for conversion in data:\n",
        "            # Perform ingestion operations\n",
        "            print(\"Ingesting click/conversion:\", conversion)\n",
        "\n",
        "    def ingest_bid_requests(self, data):\n",
        "        # Process bid requests data\n",
        "        for bid_request in data:\n",
        "            # Perform ingestion operations\n",
        "            print(\"Ingesting bid request:\", bid_request)\n",
        "\n",
        "    def generate_sample_clicks_conversions(self, num_records):\n",
        "        # Generate sample clicks/conversions data\n",
        "        conversions = []\n",
        "        for _ in range(num_records):\n",
        "            conversion = {\n",
        "                \"user_id\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=5)),\n",
        "                \"timestamp\": str(datetime.datetime.now() - datetime.timedelta(minutes=random.randint(1, 60))),\n",
        "                \"ad_campaign_id\": random.randint(1, 5),\n",
        "                \"conversion_type\": random.choice([\"sign-up\", \"purchase\", \"download\"])\n",
        "            }\n",
        "            conversions.append(conversion)\n",
        "\n",
        "        return conversions\n",
        "\n",
        "    def generate_sample_bid_requests(self, num_requests):\n",
        "        # Generate sample bid requests data\n",
        "        bid_requests = []\n",
        "        for _ in range(num_requests):\n",
        "            bid_request = {\n",
        "                \"user_id\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=5)),\n",
        "                \"auction_id\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)),\n",
        "                \"ad_targeting_criteria\": {\n",
        "                    \"age\": random.randint(18, 65),\n",
        "                    \"gender\": random.choice([\"male\", \"female\", \"other\"]),\n",
        "                    \"location\": random.choice([\"USA\", \"UK\", \"Canada\", \"Australia\"])\n",
        "                }\n",
        "            }\n",
        "            bid_requests.append(bid_request)\n",
        "\n",
        "        return bid_requests\n",
        "\n",
        "# Define the DataProcessor class\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def standardize_timestamp(self, timestamp):\n",
        "        return datetime.datetime.fromisoformat(timestamp)\n",
        "\n",
        "    def enrich_data(self, data):\n",
        "        # Placeholder for data enrichment logic\n",
        "        return data\n",
        "\n",
        "    def validate_data(self, data):\n",
        "        for item in data:\n",
        "            if 'user_id' not in item:\n",
        "                raise ValueError(\"Missing required field: user_id\")\n",
        "            if 'timestamp' not in item:\n",
        "                raise ValueError(\"Missing required field: timestamp\")\n",
        "        return data\n",
        "\n",
        "    def filter_data(self, data):\n",
        "        # Placeholder for data filtering logic\n",
        "        return data\n",
        "\n",
        "    def deduplicate_data(self, data):\n",
        "        # Placeholder for data deduplication logic\n",
        "        return data\n",
        "\n",
        "    def correlate_data(self, impressions, clicks_conversions, time_window=3600):\n",
        "        correlated_data = []\n",
        "        for impression in impressions:\n",
        "            impression_user_id = impression['user_id']\n",
        "            impression_timestamp = self.standardize_timestamp(impression['timestamp'])\n",
        "            for click_conversion in clicks_conversions:\n",
        "                click_conversion_user_id = click_conversion['user_id']\n",
        "                click_conversion_timestamp = self.standardize_timestamp(click_conversion['timestamp'])\n",
        "                if impression_user_id == click_conversion_user_id:\n",
        "                    time_difference = abs((impression_timestamp - click_conversion_timestamp).total_seconds())\n",
        "                    if time_difference <= time_window:\n",
        "                        correlated_data.append({'impression': impression, 'click_conversion': click_conversion})\n",
        "                        break  # Once a match is found, break out of the inner loop\n",
        "        return correlated_data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ingester = DataIngester()\n",
        "\n",
        "    # Example ingesting ad impressions data\n",
        "    ad_impressions_json = [\n",
        "        {\"ad_creative_id\": 1, \"user_id\": \"101\", \"timestamp\": \"2024-05-02T10:00:00\", \"website\": \"example.com\"},\n",
        "        {\"ad_creative_id\": 2, \"user_id\": \"102\", \"timestamp\": \"2024-05-02T11:00:00\", \"website\": \"example.net\"}\n",
        "    ]\n",
        "    ingester.ingest_ad_impressions(ad_impressions_json)\n",
        "\n",
        "    # Example ingesting sample clicks/conversions data\n",
        "    sample_clicks_conversions = ingester.generate_sample_clicks_conversions(10)  # Increased to 10 records\n",
        "    ingester.ingest_clicks_conversions(sample_clicks_conversions)\n",
        "\n",
        "    # Example ingesting sample bid request data\n",
        "    sample_bid_requests = ingester.generate_sample_bid_requests(3)\n",
        "    ingester.ingest_bid_requests(sample_bid_requests)\n",
        "\n",
        "    # Data processing\n",
        "    processor = DataProcessor()\n",
        "\n",
        "    # Sample data\n",
        "    ad_impressions_json = [\n",
        "        {\"ad_creative_id\": 1, \"user_id\": \"101\", \"timestamp\": \"2024-05-02T10:00:00\", \"website\": \"example.com\"},\n",
        "        {\"ad_creative_id\": 2, \"user_id\": \"102\", \"timestamp\": \"2024-05-02T11:00:00\", \"website\": \"example.net\"}\n",
        "    ]\n",
        "    clicks_conversions = [\n",
        "        {\"user_id\": \"101\", \"timestamp\": \"2024-05-02T10:05:00\", \"ad_campaign_id\": 1, \"conversion_type\": \"sign-up\"},\n",
        "        {\"user_id\": \"102\", \"timestamp\": \"2024-05-02T11:15:00\", \"ad_campaign_id\": 2, \"conversion_type\": \"purchase\"}\n",
        "    ]\n",
        "\n",
        "    # Data processing\n",
        "    standardized_impressions = [processor.standardize_timestamp(impression['timestamp']) for impression in ad_impressions_json]\n",
        "    enriched_impressions = processor.enrich_data(ad_impressions_json)\n",
        "    validated_impressions = processor.validate_data(enriched_impressions)\n",
        "    filtered_impressions = processor.filter_data(validated_impressions)\n",
        "    deduplicated_impressions = processor.deduplicate_data(filtered_impressions)\n",
        "\n",
        "    standardized_clicks_conversions = [processor.standardize_timestamp(cc['timestamp']) for cc in clicks_conversions]\n",
        "    enriched_clicks_conversions = processor.enrich_data(clicks_conversions)\n",
        "    validated_clicks_conversions = processor.validate_data(enriched_clicks_conversions)\n",
        "    filtered_clicks_conversions = processor.filter_data(validated_clicks_conversions)\n",
        "    deduplicated_clicks_conversions = processor.deduplicate_data(filtered_clicks_conversions)\n",
        "\n",
        "    # Correlation\n",
        "    correlated_data = processor.correlate_data(deduplicated_impressions, deduplicated_clicks_conversions)\n",
        "    print(\"Correlated data:\", correlated_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Data Storage and Query Performance\n"
      ],
      "metadata": {
        "id": "iIx87fyU7moF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install postgresql postgresql-contrib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiEXB7uhmE75",
        "outputId": "b1313467-6c37-4be6-fe9a-238a89b7176e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "postgresql is already the newest version (14+238).\n",
            "postgresql-contrib is already the newest version (14+238).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!service postgresql start"
      ],
      "metadata": {
        "id": "jAyWQMJEpIfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d108a70-c408-4099-8738-f79f09302a79"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psycopg2-binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtxdehN--d7G",
        "outputId": "673dac1a-2b43-429f-ad45-df3249caf3fe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (2.9.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u postgres psql -c \"CREATE DATABASE pesto_techi;\"\n",
        "!sudo -u postgres psql -c \"CREATE USER jebin12 WITH PASSWORD 'jebin@055';\"\n",
        "!sudo -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE pesto_tech TO jebin12;\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgR4l2fXUgc8",
        "outputId": "14f4a103-657a-4e82-e107-e47edc516a4b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR:  database \"pesto_techi\" already exists\n",
            "ERROR:  role \"jebin12\" already exists\n",
            "GRANT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def standardize_timestamp(self, timestamp):\n",
        "        return datetime.datetime.fromisoformat(timestamp)\n",
        "\n",
        "    def enrich_data(self, data):\n",
        "        # Placeholder for data enrichment logic\n",
        "        return data\n",
        "\n",
        "    def validate_data(self, data):\n",
        "        for item in data:\n",
        "            if 'user_id' not in item:\n",
        "                raise ValueError(\"Missing required field: user_id\")\n",
        "            if 'timestamp' not in item:\n",
        "                raise ValueError(\"Missing required field: timestamp\")\n",
        "        return data\n",
        "\n",
        "    def filter_data(self, data):\n",
        "        # Placeholder for data filtering logic\n",
        "        return data\n",
        "\n",
        "    def deduplicate_data(self, data):\n",
        "        # Placeholder for data deduplication logic\n",
        "        return data\n",
        "\n",
        "    def correlate_data(self, impressions, clicks_conversions, time_window=3600):\n",
        "        correlated_data = []\n",
        "        for impression in impressions:\n",
        "            impression_user_id = impression['user_id']\n",
        "            impression_timestamp = self.standardize_timestamp(impression['timestamp'])\n",
        "            for click_conversion in clicks_conversions:\n",
        "                click_conversion_user_id = click_conversion['user_id']\n",
        "                click_conversion_timestamp = self.standardize_timestamp(click_conversion['timestamp'])\n",
        "                if impression_user_id == click_conversion_user_id:\n",
        "                    time_difference = abs((impression_timestamp - click_conversion_timestamp).total_seconds())\n",
        "                    if time_difference <= time_window:\n",
        "                        correlated_data.append({'impression': impression, 'click_conversion': click_conversion})\n",
        "                        break  # Once a match is found, break out of the inner loop\n",
        "        return correlated_data\n",
        "\n",
        "# Creating an instance of DataProcessor\n",
        "processor = DataProcessor()\n",
        "\n",
        "# Sample data\n",
        "ad_impressions_json = [\n",
        "    {\"ad_creative_id\": 1, \"user_id\": \"101\", \"timestamp\": \"2024-05-02T10:00:00\", \"website\": \"example.com\"},\n",
        "    {\"ad_creative_id\": 2, \"user_id\": \"102\", \"timestamp\": \"2024-05-02T11:00:00\", \"website\": \"example.net\"}\n",
        "]\n",
        "clicks_conversions = [\n",
        "    {\"user_id\": \"101\", \"timestamp\": \"2024-05-02T10:05:00\", \"ad_campaign_id\": 1, \"conversion_type\": \"sign-up\"},\n",
        "    {\"user_id\": \"102\", \"timestamp\": \"2024-05-02T11:15:00\", \"ad_campaign_id\": 2, \"conversion_type\": \"purchase\"}\n",
        "]\n",
        "\n",
        "# Data processing\n",
        "standardized_impressions = [processor.standardize_timestamp(impression['timestamp']) for impression in ad_impressions_json]\n",
        "enriched_impressions = processor.enrich_data(ad_impressions_json)\n",
        "validated_impressions = processor.validate_data(enriched_impressions)\n",
        "filtered_impressions = processor.filter_data(validated_impressions)\n",
        "deduplicated_impressions = processor.deduplicate_data(filtered_impressions)\n",
        "\n",
        "standardized_clicks_conversions = [processor.standardize_timestamp(cc['timestamp']) for cc in clicks_conversions]\n",
        "enriched_clicks_conversions = processor.enrich_data(clicks_conversions)\n",
        "validated_clicks_conversions = processor.validate_data(enriched_clicks_conversions)\n",
        "filtered_clicks_conversions = processor.filter_data(validated_clicks_conversions)\n",
        "deduplicated_clicks_conversions = processor.deduplicate_data(filtered_clicks_conversions)\n",
        "\n",
        "# Correlation\n",
        "correlated_data = processor.correlate_data(deduplicated_impressions, deduplicated_clicks_conversions)\n",
        "print(\"Correlated data:\", correlated_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dS5f7pjzRGY",
        "outputId": "cb5fceac-4662-45c9-a8a0-8cde7cf68c52"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlated data: [{'impression': {'ad_creative_id': 1, 'user_id': '101', 'timestamp': '2024-05-02T10:00:00', 'website': 'example.com'}, 'click_conversion': {'user_id': '101', 'timestamp': '2024-05-02T10:05:00', 'ad_campaign_id': 1, 'conversion_type': 'sign-up'}}, {'impression': {'ad_creative_id': 2, 'user_id': '102', 'timestamp': '2024-05-02T11:00:00', 'website': 'example.net'}, 'click_conversion': {'user_id': '102', 'timestamp': '2024-05-02T11:15:00', 'ad_campaign_id': 2, 'conversion_type': 'purchase'}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the PostgreSQL database\n",
        "conn = psycopg2.connect(\n",
        "    dbname='pesto_techi',\n",
        "    user='jebin12',\n",
        "    password='jebin@055',\n",
        "    host='localhost',\n",
        "    port='5432'\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Execute SQL command to create the click_conversions table\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS click_conversions (\n",
        "        user_id VARCHAR(50),\n",
        "        timestamp TIMESTAMP,\n",
        "        ad_campaign_id INT,\n",
        "        conversion_type VARCHAR(50)\n",
        "    );\n",
        "\"\"\")\n",
        "\n",
        "# Insert correlated data into the ad_impressions and click_conversions tables\n",
        "for pair in correlated_data:\n",
        "    impression = pair['impression']\n",
        "    click_conversion = pair['click_conversion']\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO ad_impressions (ad_creative_id, user_id, timestamp, website)\n",
        "        VALUES (%s, %s, %s, %s);\n",
        "    \"\"\", (impression['ad_creative_id'], impression['user_id'], impression['timestamp'], impression['website']))\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO click_conversions (user_id, timestamp, ad_campaign_id, conversion_type)\n",
        "        VALUES (%s, %s, %s, %s);\n",
        "    \"\"\", (click_conversion['user_id'], click_conversion['timestamp'], click_conversion['ad_campaign_id'], click_conversion['conversion_type']))\n",
        "\n",
        "# Commit the transaction\n",
        "conn.commit()\n",
        "\n",
        "# Close the cursor and connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "_q82rT_f9RuZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "conn = psycopg2.connect(\n",
        "    dbname='pesto_techi',\n",
        "    user='jebin12',\n",
        "    password='jebin@055',\n",
        "    host='localhost',\n",
        "    port='5432'\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Execute SQL queries to fetch data from ad_impressions and click_conversions tables\n",
        "cur.execute(\"SELECT * FROM ad_impressions;\")\n",
        "impression_rows = cur.fetchall()\n",
        "\n",
        "cur.execute(\"SELECT * FROM click_conversions;\")\n",
        "conversion_rows = cur.fetchall()\n",
        "\n",
        "# Print fetched data from ad_impressions table\n",
        "print(\"Data from ad_impressions table:\")\n",
        "for row in impression_rows:\n",
        "    print(row)\n",
        "\n",
        "# Print fetched data from click_conversions table\n",
        "print(\"\\nData from click_conversions table:\")\n",
        "for row in conversion_rows:\n",
        "    print(row)\n",
        "\n",
        "# Close the cursor and connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJMcrw_NW-kW",
        "outputId": "5339639a-46d9-4375-d50b-1337d1c42b1e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data from ad_impressions table:\n",
            "(1, '101', datetime.datetime(2024, 5, 2, 10, 0), 'example.com')\n",
            "(2, '102', datetime.datetime(2024, 5, 2, 11, 0), 'example.net')\n",
            "(1, '101', datetime.datetime(2024, 5, 2, 10, 0), 'example.com')\n",
            "(2, '102', datetime.datetime(2024, 5, 2, 11, 0), 'example.net')\n",
            "(1, '101', datetime.datetime(2024, 5, 2, 10, 0), 'example.com')\n",
            "(2, '102', datetime.datetime(2024, 5, 2, 11, 0), 'example.net')\n",
            "(1, '101', datetime.datetime(2024, 5, 2, 10, 0), 'example.com')\n",
            "(2, '102', datetime.datetime(2024, 5, 2, 11, 0), 'example.net')\n",
            "\n",
            "Data from click_conversions table:\n",
            "('101', datetime.datetime(2024, 5, 2, 10, 5), 1, 'sign-up')\n",
            "('102', datetime.datetime(2024, 5, 2, 11, 15), 2, 'purchase')\n",
            "('101', datetime.datetime(2024, 5, 2, 10, 5), 1, 'sign-up')\n",
            "('102', datetime.datetime(2024, 5, 2, 11, 15), 2, 'purchase')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PGPASSWORD=jebin@055 psql -h localhost -U jebin12 -d pesto_techi -c \"\\copy (SELECT * FROM ad_impressions) TO '/content/ad_impressions.csv' CSV HEADER;\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKAxXc-eeYK2",
        "outputId": "e1989686-c238-44aa-9635-1bacf2389d04"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COPY 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEQIWtF6mWJl",
        "outputId": "1923526a-47f9-4160-bbc7-7f557fd4dfcd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=IHfbkMudR191d8rrd75Q8lpkFs1VGy&prompt=consent&token_usage=remote&access_type=offline&code_challenge=ok1NVbPti-r3vx-zNftaNg41BMPVnq4cCdqn-AbVjyo&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AdLIrYfd0-cGVhhKSiHJ3dNQb-ijSb2YhsclAk-mobgnXF3NfX4NKKvOXMd93Jtdh9qQRQ\n",
            "\n",
            "You are now logged in as [jebineabraham94@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project jebine\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuKajHkLm4E2",
        "outputId": "806b267d-fec0-42ff-f4e8-c4b2d809c73f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp /content/ad_impressions.csv gs://datas_for_projects/destination/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtIquXfNn46T",
        "outputId": "92afe75c-5f42-4172-8b3e-856329ccf086"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/ad_impressions.csv [Content-Type=text/csv]...\n",
            "-\n",
            "Operation completed over 1 objects/345.0 B.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://datas_for_projects/destination/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2nMU6wuo1mo",
        "outputId": "7eefc368-f0ff-4fa6-f8f8-194bff7de8f2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://datas_for_projects/destination/ad_impressions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Error Handling and Monitoring:"
      ],
      "metadata": {
        "id": "pb0kusNP76Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def enrich_data(self, data):\n",
        "        # Perform data enrichment, e.g., add additional information based on user_id or ad_creative_id\n",
        "        for item in data:\n",
        "            # Example: Enrich with user demographics based on user_id\n",
        "            item['user_demographics'] = self.get_user_demographics(item['user_id'])\n",
        "        return data\n",
        "\n",
        "    def aggregate_data(self, data):\n",
        "        # Perform data aggregation, e.g., calculate total impressions or conversions per ad campaign\n",
        "        aggregated_data = {}\n",
        "        for item in data:\n",
        "            ad_campaign_id = item['ad_campaign_id']\n",
        "            if ad_campaign_id not in aggregated_data:\n",
        "                aggregated_data[ad_campaign_id] = {'total_impressions': 0, 'total_conversions': 0}\n",
        "            aggregated_data[ad_campaign_id]['total_impressions'] += 1\n",
        "            if 'conversion_type' in item:\n",
        "                aggregated_data[ad_campaign_id]['total_conversions'] += 1\n",
        "        return aggregated_data\n",
        "\n",
        "    def get_user_demographics(self, user_id):\n",
        "        # Placeholder for user demographics retrieval\n",
        "        # You can implement logic to fetch user demographics from a database or external API\n",
        "        return {\"age\": 25, \"gender\": \"male\", \"location\": \"USA\"}\n",
        "\n",
        "\n",
        "# Creating an instance of DataProcessor\n",
        "processor = DataProcessor()\n",
        "\n",
        "# Example data\n",
        "clicks_conversions = [\n",
        "    {\"user_id\": \"101\", \"timestamp\": \"2024-05-02T10:05:00\", \"ad_campaign_id\": 1, \"conversion_type\": \"sign-up\"},\n",
        "    {\"user_id\": \"102\", \"timestamp\": \"2024-05-02T11:15:00\", \"ad_campaign_id\": 2, \"conversion_type\": \"purchase\"}\n",
        "]\n",
        "\n",
        "# Enriching data\n",
        "enriched_clicks_conversions = processor.enrich_data(clicks_conversions)\n",
        "\n",
        "# Aggregating data\n",
        "aggregated_data = processor.aggregate_data(enriched_clicks_conversions)\n",
        "print(\"Aggregated data:\", aggregated_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYh0EwRvqQRH",
        "outputId": "f2a1d97f-1f64-4905-c63a-935d81b0a7da"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregated data: {1: {'total_impressions': 1, 'total_conversions': 1}, 2: {'total_impressions': 1, 'total_conversions': 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the PostgreSQL database\n",
        "conn = psycopg2.connect(\n",
        "    dbname='pesto_techi',\n",
        "    user='jebin12',\n",
        "    password='jebin@055',\n",
        "    host='localhost',\n",
        "    port='5432'\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Example: Adding index to ad_impressions table for faster retrieval\n",
        "cur.execute(\"CREATE INDEX ON ad_impressions(ad_creative_id);\")\n",
        "\n",
        "# Commit the transaction\n",
        "conn.commit()\n",
        "\n",
        "# Close the cursor and connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "4w9HaTBtrm35"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Define pipeline tasks as shell commands\n",
        "tasks = [\n",
        "    \"python data_processing_script.py\",  # Run data processing script\n",
        "    \"python analysis_script.py\",          # Run analysis script\n",
        "    \"python visualization_script.py\"      # Run visualization script\n",
        "]\n",
        "\n",
        "# Execute pipeline tasks sequentially\n",
        "for task in tasks:\n",
        "    subprocess.run(task, shell=True)\n"
      ],
      "metadata": {
        "id": "x_1VAAB1sQkT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install virtualenv\n",
        "!pip install virtualenv\n",
        "\n",
        "# Create a new directory for your project\n",
        "!mkdir airflow_project\n",
        "\n",
        "# Navigate to the project directory\n",
        "%cd airflow_project\n",
        "\n",
        "# Create a virtual environment\n",
        "!virtualenv venv\n",
        "\n",
        "# Activate the virtual environment\n",
        "!source venv/bin/activate\n",
        "\n",
        "# Install Apache Airflow in the virtual environment\n",
        "!pip install apache-airflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpJRGAF2t8jD",
        "outputId": "f5359bd3-7402-4b6f-965c-f80471d62466"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.26.1-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.13.4)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.2.1)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.8 virtualenv-20.26.1\n",
            "/content/airflow_project\n",
            "created virtual environment CPython3.10.12.final.0-64 in 1268ms\n",
            "  creator CPython3Posix(dest=/content/airflow_project/venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==24.0, setuptools==69.5.1, wheel==0.43.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "Collecting apache-airflow\n",
            "  Using cached apache_airflow-2.9.0-py3-none-any.whl (13.3 MB)\n",
            "Collecting alembic<2.0,>=1.13.1 (from apache-airflow)\n",
            "  Using cached alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "Collecting argcomplete>=1.10 (from apache-airflow)\n",
            "  Using cached argcomplete-3.3.0-py3-none-any.whl (42 kB)\n",
            "Collecting asgiref (from apache-airflow)\n",
            "  Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: attrs>=22.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (23.2.0)\n",
            "Collecting blinker>=1.6.2 (from apache-airflow)\n",
            "  Using cached blinker-1.8.1-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: colorlog<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (4.8.0)\n",
            "Requirement already satisfied: configupdater>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (3.2)\n",
            "Collecting connexion[flask]<3.0,>=2.10.0 (from apache-airflow)\n",
            "  Using cached connexion-2.14.2-py2.py3-none-any.whl (95 kB)\n",
            "Requirement already satisfied: cron-descriptor>=1.2.24 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (1.4.3)\n",
            "Collecting croniter>=2.0.2 (from apache-airflow)\n",
            "  Using cached croniter-2.0.5-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: cryptography>=39.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (42.0.5)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (1.2.14)\n",
            "Requirement already satisfied: dill>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.3.8)\n",
            "Collecting flask-caching>=1.5.0 (from apache-airflow)\n",
            "  Using cached Flask_Caching-2.2.0-py3-none-any.whl (28 kB)\n",
            "Collecting flask-session<0.6,>=0.4.0 (from apache-airflow)\n",
            "  Using cached flask_session-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Collecting flask-wtf>=0.15 (from apache-airflow)\n",
            "  Using cached flask_wtf-1.2.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: flask<2.3,>=2.2 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.2.5)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2024.3.1)\n",
            "Requirement already satisfied: google-re2>=1.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (1.1.20240501)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (22.0.0)\n",
            "Collecting httpx (from apache-airflow)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Requirement already satisfied: importlib_metadata>=6.5 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (7.0.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (4.19.2)\n",
            "Requirement already satisfied: lazy-object-proxy in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (1.10.0)\n",
            "Requirement already satisfied: linkify-it-py>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.0.3)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (3.0.0)\n",
            "Requirement already satisfied: markupsafe>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.1.5)\n",
            "Collecting marshmallow-oneofschema>=2.0.1 (from apache-airflow)\n",
            "  Using cached marshmallow_oneofschema-3.1.1-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: mdit-py-plugins>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.4.0)\n",
            "Collecting opentelemetry-api>=1.15.0 (from apache-airflow)\n",
            "  Using cached opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp-1.24.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (24.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.12.1)\n",
            "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow)\n",
            "  Using cached pendulum-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
            "Requirement already satisfied: pluggy>=1.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (1.5.0)\n",
            "Requirement already satisfied: psutil>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (5.9.5)\n",
            "Requirement already satisfied: pygments>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.16.1)\n",
            "Requirement already satisfied: pyjwt>=2.0.0 in /usr/lib/python3/dist-packages (from apache-airflow) (2.3.0)\n",
            "Requirement already satisfied: python-daemon>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.3 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.8.2)\n",
            "Collecting python-nvd3>=0.15.0 (from apache-airflow)\n",
            "  Using cached python_nvd3-0.16.0-py3-none-any.whl\n",
            "Requirement already satisfied: python-slugify>=5.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (8.0.4)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.31.0)\n",
            "Requirement already satisfied: rfc3339-validator>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.1.4)\n",
            "Collecting rich-argparse>=1.0.0 (from apache-airflow)\n",
            "  Using cached rich_argparse-1.4.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: rich>=12.4.4 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (13.7.1)\n",
            "Requirement already satisfied: setproctitle>=1.1.8 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (1.3.3)\n",
            "Requirement already satisfied: sqlalchemy<2.0,>=1.4.36 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (1.4.52)\n",
            "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow)\n",
            "  Using cached SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.2.0,>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (8.2.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.4.0)\n",
            "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.14.1)\n",
            "Requirement already satisfied: universal-pathlib>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (0.2.2)\n",
            "Requirement already satisfied: werkzeug<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow) (2.2.3)\n",
            "Collecting apache-airflow-providers-common-io (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_common_io-1.3.1-py3-none-any.whl (16 kB)\n",
            "Collecting apache-airflow-providers-common-sql (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_common_sql-1.12.0-py3-none-any.whl (45 kB)\n",
            "Collecting apache-airflow-providers-fab>=1.0.2 (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_fab-1.0.4-py3-none-any.whl (78 kB)\n",
            "Collecting apache-airflow-providers-ftp (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_ftp-3.8.0-py3-none-any.whl (19 kB)\n",
            "Collecting apache-airflow-providers-http (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_http-4.10.1-py3-none-any.whl (27 kB)\n",
            "Collecting apache-airflow-providers-imap (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_imap-3.5.0-py3-none-any.whl (17 kB)\n",
            "Collecting apache-airflow-providers-smtp (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_smtp-1.6.1-py3-none-any.whl (22 kB)\n",
            "Collecting apache-airflow-providers-sqlite (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_sqlite-3.7.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic<2.0,>=1.13.1->apache-airflow) (4.11.0)\n",
            "Collecting flask-appbuilder==4.4.1 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_AppBuilder-4.4.1-py3-none-any.whl (2.2 MB)\n",
            "Collecting flask-login>=0.6.2 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
            "Collecting apispec[yaml]<7,>=6.0.0 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached apispec-6.6.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: colorama<1,>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.4.6)\n",
            "Requirement already satisfied: click<9,>=8 in /usr/local/lib/python3.10/dist-packages (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (8.1.7)\n",
            "Requirement already satisfied: email-validator>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.1.1)\n",
            "Collecting Flask-Babel<3,>=1 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting Flask-Limiter<4,>3 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_Limiter-3.6.0-py3-none-any.whl (28 kB)\n",
            "Collecting Flask-SQLAlchemy<3,>=2.4 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_SQLAlchemy-2.5.1-py2.py3-none-any.whl (17 kB)\n",
            "Collecting Flask-JWT-Extended<5.0.0,>=4.0.0 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: marshmallow<4,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.21.2)\n",
            "Collecting marshmallow-sqlalchemy<0.29.0,>=0.22.0 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached marshmallow_sqlalchemy-0.28.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: prison<1.0.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (0.2.1)\n",
            "Collecting sqlalchemy-utils<1,>=0.32.21 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
            "Requirement already satisfied: WTForms<4 in /usr/local/lib/python3.10/dist-packages (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (3.1.2)\n",
            "Requirement already satisfied: clickclick<21,>=1.2 in /usr/local/lib/python3.10/dist-packages (from connexion[flask]<3.0,>=2.10.0->apache-airflow) (20.10.2)\n",
            "Requirement already satisfied: PyYAML<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from connexion[flask]<3.0,>=2.10.0->apache-airflow) (6.0.1)\n",
            "Requirement already satisfied: inflection<0.6,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from connexion[flask]<3.0,>=2.10.0->apache-airflow) (0.5.1)\n",
            "Requirement already satisfied: pytz>2021.1 in /usr/local/lib/python3.10/dist-packages (from croniter>=2.0.2->apache-airflow) (2023.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=39.0.0->apache-airflow) (1.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->apache-airflow) (1.14.1)\n",
            "Requirement already satisfied: cachelib<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from flask-caching>=1.5.0->apache-airflow) (0.9.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata>=6.5->apache-airflow) (3.18.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->apache-airflow) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->apache-airflow) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->apache-airflow) (0.18.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py>=2.0.0->apache-airflow) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.1.0->apache-airflow) (0.1.2)\n",
            "Requirement already satisfied: tzdata>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pendulum<4.0,>=2.1.2->apache-airflow) (2024.1)\n",
            "Collecting time-machine>=2.6.0 (from pendulum<4.0,>=2.1.2->apache-airflow)\n",
            "  Using cached time_machine-2.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from python-daemon>=3.0.0->apache-airflow) (0.18.1)\n",
            "Requirement already satisfied: setuptools>=62.4.0 in /usr/local/lib/python3.10/dist-packages (from python-daemon>=3.0.0->apache-airflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.3->apache-airflow) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify>=5.0->apache-airflow) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->apache-airflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->apache-airflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->apache-airflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->apache-airflow) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0,>=1.4.36->apache-airflow) (3.0.3)\n",
            "Requirement already satisfied: more-itertools>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-airflow-providers-common-sql->apache-airflow) (10.1.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from apache-airflow-providers-common-sql->apache-airflow) (0.5.0)\n",
            "Requirement already satisfied: aiohttp>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from apache-airflow-providers-http->apache-airflow) (3.9.5)\n",
            "Collecting requests_toolbelt (from apache-airflow-providers-http->apache-airflow)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->apache-airflow) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->apache-airflow) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->apache-airflow) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->apache-airflow) (0.14.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.24.0 (from opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.24.0 (from opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow) (1.62.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow) (1.24.0)\n",
            "Collecting opentelemetry-sdk~=1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.24.0->opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow) (4.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=39.0.0->apache-airflow) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->apache-airflow) (1.2.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=1.0.5->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.6.1)\n",
            "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.10/dist-packages (from Flask-Babel<3,>=1->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (2.14.0)\n",
            "Collecting limits>=2.8 (from Flask-Limiter<4,>3->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached limits-3.11.0-py3-none-any.whl (45 kB)\n",
            "Requirement already satisfied: ordered-set<5,>4 in /usr/local/lib/python3.10/dist-packages (from Flask-Limiter<4,>3->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (4.1.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk~=1.24.0->opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow) (0.45b0)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.10/dist-packages (from limits>=2.8->Flask-Limiter<4,>3->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow) (6.4.0)\n",
            "Installing collected packages: blinker, asgiref, argcomplete, apispec, time-machine, sqlalchemy-utils, sqlalchemy-jsonfield, requests_toolbelt, python-nvd3, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, marshmallow-sqlalchemy, marshmallow-oneofschema, limits, httpx, croniter, alembic, rich-argparse, pendulum, opentelemetry-sdk, flask-wtf, Flask-SQLAlchemy, flask-session, flask-login, Flask-Limiter, Flask-JWT-Extended, flask-caching, Flask-Babel, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, flask-appbuilder, connexion, opentelemetry-exporter-otlp, apache-airflow-providers-common-sql, apache-airflow-providers-sqlite, apache-airflow-providers-smtp, apache-airflow-providers-imap, apache-airflow-providers-http, apache-airflow-providers-ftp, apache-airflow-providers-fab, apache-airflow-providers-common-io, apache-airflow\n",
            "  Attempting uninstall: blinker\n",
            "    Found existing installation: blinker 1.4\n",
            "\u001b[31mERROR: Cannot uninstall 'blinker'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-airflow --ignore-installed blinker\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o5a0fketuqdu",
        "outputId": "f819863a-9f8a-4abb-9c7f-b92739aaa29b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache-airflow\n",
            "  Using cached apache_airflow-2.9.0-py3-none-any.whl (13.3 MB)\n",
            "Collecting blinker\n",
            "  Using cached blinker-1.8.1-py3-none-any.whl (9.5 kB)\n",
            "Collecting alembic<2.0,>=1.13.1 (from apache-airflow)\n",
            "  Using cached alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "Collecting argcomplete>=1.10 (from apache-airflow)\n",
            "  Using cached argcomplete-3.3.0-py3-none-any.whl (42 kB)\n",
            "Collecting asgiref (from apache-airflow)\n",
            "  Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting attrs>=22.1.0 (from apache-airflow)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog<5.0,>=4.0.2 (from apache-airflow)\n",
            "  Using cached colorlog-4.8.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting configupdater>=3.1.1 (from apache-airflow)\n",
            "  Using cached ConfigUpdater-3.2-py2.py3-none-any.whl (34 kB)\n",
            "Collecting connexion[flask]<3.0,>=2.10.0 (from apache-airflow)\n",
            "  Using cached connexion-2.14.2-py2.py3-none-any.whl (95 kB)\n",
            "Collecting cron-descriptor>=1.2.24 (from apache-airflow)\n",
            "  Using cached cron_descriptor-1.4.3-py3-none-any.whl (49 kB)\n",
            "Collecting croniter>=2.0.2 (from apache-airflow)\n",
            "  Using cached croniter-2.0.5-py2.py3-none-any.whl (20 kB)\n",
            "Collecting cryptography>=39.0.0 (from apache-airflow)\n",
            "  Downloading cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated>=1.2.13 (from apache-airflow)\n",
            "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dill>=0.2.2 (from apache-airflow)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Collecting flask-caching>=1.5.0 (from apache-airflow)\n",
            "  Using cached Flask_Caching-2.2.0-py3-none-any.whl (28 kB)\n",
            "Collecting flask-session<0.6,>=0.4.0 (from apache-airflow)\n",
            "  Using cached flask_session-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Collecting flask-wtf>=0.15 (from apache-airflow)\n",
            "  Using cached flask_wtf-1.2.1-py3-none-any.whl (12 kB)\n",
            "Collecting flask<2.3,>=2.2 (from apache-airflow)\n",
            "  Downloading Flask-2.2.5-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec>=2023.10.0 (from apache-airflow)\n",
            "  Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "Collecting google-re2>=1.0 (from apache-airflow)\n",
            "  Using cached google_re2-1.1.20240501-1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (517 kB)\n",
            "Collecting gunicorn>=20.1.0 (from apache-airflow)\n",
            "  Using cached gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "Collecting httpx (from apache-airflow)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Collecting importlib_metadata>=6.5 (from apache-airflow)\n",
            "  Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting itsdangerous>=2.0 (from apache-airflow)\n",
            "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting jinja2>=3.0.0 (from apache-airflow)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonschema>=4.18.0 (from apache-airflow)\n",
            "  Downloading jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lazy-object-proxy (from apache-airflow)\n",
            "  Using cached lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68 kB)\n",
            "Collecting linkify-it-py>=2.0.0 (from apache-airflow)\n",
            "  Downloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting lockfile>=0.12.2 (from apache-airflow)\n",
            "  Using cached lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting markdown-it-py>=2.1.0 (from apache-airflow)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markupsafe>=1.1.1 (from apache-airflow)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting marshmallow-oneofschema>=2.0.1 (from apache-airflow)\n",
            "  Using cached marshmallow_oneofschema-3.1.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting mdit-py-plugins>=0.3.0 (from apache-airflow)\n",
            "  Downloading mdit_py_plugins-0.4.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.15.0 (from apache-airflow)\n",
            "  Using cached opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp-1.24.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting packaging>=14.0 (from apache-airflow)\n",
            "  Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathspec>=0.9.0 (from apache-airflow)\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Collecting pendulum<4.0,>=2.1.2 (from apache-airflow)\n",
            "  Using cached pendulum-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
            "Collecting pluggy>=1.0 (from apache-airflow)\n",
            "  Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
            "Collecting psutil>=4.2.0 (from apache-airflow)\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments>=2.0.1 (from apache-airflow)\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt>=2.0.0 (from apache-airflow)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting python-daemon>=3.0.0 (from apache-airflow)\n",
            "  Using cached python_daemon-3.0.1-py3-none-any.whl (31 kB)\n",
            "Collecting python-dateutil>=2.3 (from apache-airflow)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-nvd3>=0.15.0 (from apache-airflow)\n",
            "  Using cached python_nvd3-0.16.0-py3-none-any.whl\n",
            "Collecting python-slugify>=5.0 (from apache-airflow)\n",
            "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
            "Collecting requests<3,>=2.27.0 (from apache-airflow)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3339-validator>=0.1.4 (from apache-airflow)\n",
            "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rich-argparse>=1.0.0 (from apache-airflow)\n",
            "  Using cached rich_argparse-1.4.0-py3-none-any.whl (19 kB)\n",
            "Collecting rich>=12.4.4 (from apache-airflow)\n",
            "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle>=1.1.8 (from apache-airflow)\n",
            "  Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting sqlalchemy<2.0,>=1.4.36 (from apache-airflow)\n",
            "  Using cached SQLAlchemy-1.4.52-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "Collecting sqlalchemy-jsonfield>=1.0 (from apache-airflow)\n",
            "  Using cached SQLAlchemy_JSONField-1.0.2-py3-none-any.whl (10 kB)\n",
            "Collecting tabulate>=0.7.5 (from apache-airflow)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting tenacity!=8.2.0,>=6.2.0 (from apache-airflow)\n",
            "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Collecting termcolor>=1.1.0 (from apache-airflow)\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Collecting unicodecsv>=0.14.1 (from apache-airflow)\n",
            "  Using cached unicodecsv-0.14.1-py3-none-any.whl\n",
            "Collecting universal-pathlib>=0.2.2 (from apache-airflow)\n",
            "  Using cached universal_pathlib-0.2.2-py3-none-any.whl (46 kB)\n",
            "Collecting werkzeug<3,>=2.0 (from apache-airflow)\n",
            "  Using cached werkzeug-2.3.8-py3-none-any.whl (242 kB)\n",
            "Collecting apache-airflow-providers-common-io (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_common_io-1.3.1-py3-none-any.whl (16 kB)\n",
            "Collecting apache-airflow-providers-common-sql (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_common_sql-1.12.0-py3-none-any.whl (45 kB)\n",
            "Collecting apache-airflow-providers-fab>=1.0.2 (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_fab-1.0.4-py3-none-any.whl (78 kB)\n",
            "Collecting apache-airflow-providers-ftp (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_ftp-3.8.0-py3-none-any.whl (19 kB)\n",
            "Collecting apache-airflow-providers-http (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_http-4.10.1-py3-none-any.whl (27 kB)\n",
            "Collecting apache-airflow-providers-imap (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_imap-3.5.0-py3-none-any.whl (17 kB)\n",
            "Collecting apache-airflow-providers-smtp (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_smtp-1.6.1-py3-none-any.whl (22 kB)\n",
            "Collecting apache-airflow-providers-sqlite (from apache-airflow)\n",
            "  Using cached apache_airflow_providers_sqlite-3.7.1-py3-none-any.whl (13 kB)\n",
            "Collecting Mako (from alembic<2.0,>=1.13.1->apache-airflow)\n",
            "  Using cached Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "Collecting typing-extensions>=4 (from alembic<2.0,>=1.13.1->apache-airflow)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting flask-appbuilder==4.4.1 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_AppBuilder-4.4.1-py3-none-any.whl (2.2 MB)\n",
            "Collecting flask-login>=0.6.2 (from apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
            "Collecting apispec[yaml]<7,>=6.0.0 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached apispec-6.6.1-py3-none-any.whl (30 kB)\n",
            "Collecting colorama<1,>=0.3.9 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting click<9,>=8 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email-validator>=1.0.5 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting Flask-Babel<3,>=1 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting Flask-Limiter<4,>3 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_Limiter-3.6.0-py3-none-any.whl (28 kB)\n",
            "Collecting Flask-SQLAlchemy<3,>=2.4 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_SQLAlchemy-2.5.1-py2.py3-none-any.whl (17 kB)\n",
            "Collecting Flask-JWT-Extended<5.0.0,>=4.0.0 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting marshmallow<4,>=3.18.0 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "Collecting marshmallow-sqlalchemy<0.29.0,>=0.22.0 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached marshmallow_sqlalchemy-0.28.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting prison<1.0.0,>=0.2.1 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached prison-0.2.1-py2.py3-none-any.whl (5.8 kB)\n",
            "Collecting sqlalchemy-utils<1,>=0.32.21 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached SQLAlchemy_Utils-0.41.2-py3-none-any.whl (93 kB)\n",
            "Collecting WTForms<4 (from flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached wtforms-3.1.2-py3-none-any.whl (145 kB)\n",
            "Collecting clickclick<21,>=1.2 (from connexion[flask]<3.0,>=2.10.0->apache-airflow)\n",
            "  Using cached clickclick-20.10.2-py2.py3-none-any.whl (7.4 kB)\n",
            "Collecting PyYAML<7,>=5.1 (from connexion[flask]<3.0,>=2.10.0->apache-airflow)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflection<0.6,>=0.3.1 (from connexion[flask]<3.0,>=2.10.0->apache-airflow)\n",
            "  Using cached inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Collecting werkzeug<3,>=2.0 (from apache-airflow)\n",
            "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "Collecting pytz>2021.1 (from croniter>=2.0.2->apache-airflow)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cffi>=1.12 (from cryptography>=39.0.0->apache-airflow)\n",
            "  Using cached cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.13->apache-airflow)\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachelib<0.10.0,>=0.9.0 (from flask-caching>=1.5.0->apache-airflow)\n",
            "  Using cached cachelib-0.9.0-py3-none-any.whl (15 kB)\n",
            "Collecting zipp>=0.5 (from importlib_metadata>=6.5->apache-airflow)\n",
            "  Downloading zipp-3.18.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->apache-airflow)\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->apache-airflow)\n",
            "  Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->apache-airflow)\n",
            "  Downloading rpds_py-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uc-micro-py (from linkify-it-py>=2.0.0->apache-airflow)\n",
            "  Downloading uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.1.0->apache-airflow)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting importlib_metadata>=6.5 (from apache-airflow)\n",
            "  Using cached importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting tzdata>=2020.1 (from pendulum<4.0,>=2.1.2->apache-airflow)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting time-machine>=2.6.0 (from pendulum<4.0,>=2.1.2->apache-airflow)\n",
            "  Using cached time_machine-2.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
            "Collecting docutils (from python-daemon>=3.0.0->apache-airflow)\n",
            "  Downloading docutils-0.21.2-py3-none-any.whl (587 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.4/587.4 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools>=62.4.0 (from python-daemon>=3.0.0->apache-airflow)\n",
            "  Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.3->apache-airflow)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting text-unidecode>=1.3 (from python-slugify>=5.0->apache-airflow)\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests<3,>=2.27.0->apache-airflow)\n",
            "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests<3,>=2.27.0->apache-airflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests<3,>=2.27.0->apache-airflow)\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3,>=2.27.0->apache-airflow)\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting greenlet!=0.4.17 (from sqlalchemy<2.0,>=1.4.36->apache-airflow)\n",
            "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting more-itertools>=9.0.0 (from apache-airflow-providers-common-sql->apache-airflow)\n",
            "  Downloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlparse>=0.4.2 (from apache-airflow-providers-common-sql->apache-airflow)\n",
            "  Downloading sqlparse-0.5.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp>=3.9.2 (from apache-airflow-providers-http->apache-airflow)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests_toolbelt (from apache-airflow-providers-http->apache-airflow)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Collecting anyio (from httpx->apache-airflow)\n",
            "  Downloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.* (from httpx->apache-airflow)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Collecting sniffio (from httpx->apache-airflow)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->apache-airflow)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.24.0 (from opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.24.0 (from opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\n",
            "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/229.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<2.0.0,>=1.0.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Downloading grpcio-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "Collecting opentelemetry-sdk~=1.24.0 (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "Collecting protobuf<5.0,>=3.19 (from opentelemetry-proto==1.24.0->opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp>=3.9.2->apache-airflow-providers-http->apache-airflow)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting pycparser (from cffi>=1.12->cryptography>=39.0.0->apache-airflow)\n",
            "  Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio->httpx->apache-airflow)\n",
            "  Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=1.0.5->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "Collecting Babel>=2.3 (from Flask-Babel<3,>=1->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Downloading Babel-2.14.0-py3-none-any.whl (11.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting limits>=2.8 (from Flask-Limiter<4,>3->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached limits-3.11.0-py3-none-any.whl (45 kB)\n",
            "Collecting ordered-set<5,>4 (from Flask-Limiter<4,>3->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-sdk~=1.24.0->opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->apache-airflow)\n",
            "  Using cached opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting importlib-resources>=1.3 (from limits>=2.8->Flask-Limiter<4,>3->flask-appbuilder==4.4.1->apache-airflow-providers-fab>=1.0.2->apache-airflow)\n",
            "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
            "Installing collected packages: unicodecsv, text-unidecode, pytz, lockfile, cron-descriptor, colorlog, zipp, wrapt, urllib3, uc-micro-py, tzdata, typing-extensions, termcolor, tenacity, tabulate, sqlparse, sniffio, six, setuptools, setproctitle, rpds-py, PyYAML, python-slugify, pyjwt, pygments, pycparser, psutil, protobuf, pluggy, pathspec, packaging, ordered-set, opentelemetry-semantic-conventions, multidict, more-itertools, mdurl, markupsafe, lazy-object-proxy, itsdangerous, inflection, importlib-resources, idna, h11, grpcio, greenlet, google-re2, fsspec, frozenlist, exceptiongroup, docutils, dnspython, dill, configupdater, colorama, click, charset-normalizer, certifi, cachelib, blinker, Babel, attrs, async-timeout, argcomplete, yarl, WTForms, werkzeug, universal-pathlib, sqlalchemy, rfc3339-validator, requests, referencing, python-dateutil, python-daemon, prison, opentelemetry-proto, marshmallow, markdown-it-py, Mako, linkify-it-py, jinja2, importlib_metadata, httpcore, gunicorn, googleapis-common-protos, email-validator, deprecated, clickclick, cffi, asgiref, apispec, anyio, aiosignal, time-machine, sqlalchemy-utils, sqlalchemy-jsonfield, rich, requests_toolbelt, python-nvd3, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, mdit-py-plugins, marshmallow-sqlalchemy, marshmallow-oneofschema, limits, jsonschema-specifications, httpx, flask, cryptography, croniter, alembic, aiohttp, rich-argparse, pendulum, opentelemetry-sdk, jsonschema, flask-wtf, Flask-SQLAlchemy, flask-session, flask-login, Flask-Limiter, Flask-JWT-Extended, flask-caching, Flask-Babel, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, flask-appbuilder, connexion, opentelemetry-exporter-otlp, apache-airflow-providers-common-sql, apache-airflow-providers-sqlite, apache-airflow-providers-smtp, apache-airflow-providers-imap, apache-airflow-providers-http, apache-airflow-providers-ftp, apache-airflow-providers-fab, apache-airflow-providers-common-io, apache-airflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "sphinx 5.0.2 requires docutils<0.19,>=0.14, but you have docutils 0.21.2 which is incompatible.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.52 which is incompatible.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.3.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.16.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Babel-2.14.0 Flask-Babel-2.0.0 Flask-JWT-Extended-4.6.0 Flask-Limiter-3.6.0 Flask-SQLAlchemy-2.5.1 Mako-1.3.3 PyYAML-6.0.1 WTForms-3.1.2 aiohttp-3.9.5 aiosignal-1.3.1 alembic-1.13.1 anyio-3.7.1 apache-airflow-2.9.0 apache-airflow-providers-common-io-1.3.1 apache-airflow-providers-common-sql-1.12.0 apache-airflow-providers-fab-1.0.4 apache-airflow-providers-ftp-3.8.0 apache-airflow-providers-http-4.10.1 apache-airflow-providers-imap-3.5.0 apache-airflow-providers-smtp-1.6.1 apache-airflow-providers-sqlite-3.7.1 apispec-6.6.1 argcomplete-3.3.0 asgiref-3.8.1 async-timeout-4.0.3 attrs-23.2.0 blinker-1.8.1 cachelib-0.9.0 certifi-2024.2.2 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 clickclick-20.10.2 colorama-0.4.6 colorlog-4.8.0 configupdater-3.2 connexion-2.14.2 cron-descriptor-1.4.3 croniter-2.0.5 cryptography-42.0.5 deprecated-1.2.14 dill-0.3.8 dnspython-2.6.1 docutils-0.18.1 email-validator-2.1.1 exceptiongroup-1.2.1 flask-2.2.5 flask-appbuilder-4.4.1 flask-caching-2.2.0 flask-login-0.6.3 flask-session-0.5.0 flask-wtf-1.2.1 frozenlist-1.4.1 fsspec-2024.3.1 google-re2-1.1.20240501 googleapis-common-protos-1.63.0 greenlet-3.0.3 grpcio-1.62.2 gunicorn-22.0.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 importlib-resources-6.4.0 importlib_metadata-7.0.0 inflection-0.5.1 itsdangerous-2.2.0 jinja2-3.1.3 jsonschema-4.19.2 jsonschema-specifications-2023.12.1 lazy-object-proxy-1.10.0 limits-3.11.0 linkify-it-py-2.0.3 lockfile-0.12.2 markdown-it-py-3.0.0 markupsafe-2.1.5 marshmallow-3.21.2 marshmallow-oneofschema-3.1.1 marshmallow-sqlalchemy-0.28.2 mdit-py-plugins-0.4.0 mdurl-0.1.2 more-itertools-10.1.0 multidict-6.0.5 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-exporter-otlp-proto-http-1.24.0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 ordered-set-4.1.0 packaging-24.0 pathspec-0.12.1 pendulum-3.0.0 pluggy-1.5.0 prison-0.2.1 protobuf-3.20.3 psutil-5.9.5 pycparser-2.22 pygments-2.16.1 pyjwt-2.8.0 python-daemon-3.0.1 python-dateutil-2.8.2 python-nvd3-0.16.0 python-slugify-8.0.4 pytz-2023.4 referencing-0.35.0 requests-2.31.0 requests_toolbelt-1.0.0 rfc3339-validator-0.1.4 rich-13.7.1 rich-argparse-1.4.0 rpds-py-0.18.0 setproctitle-1.3.3 setuptools-67.7.2 six-1.16.0 sniffio-1.3.1 sqlalchemy-1.4.52 sqlalchemy-jsonfield-1.0.2 sqlalchemy-utils-0.41.2 sqlparse-0.5.0 tabulate-0.9.0 tenacity-8.2.3 termcolor-2.4.0 text-unidecode-1.3 time-machine-2.14.1 typing-extensions-4.11.0 tzdata-2024.1 uc-micro-py-1.0.3 unicodecsv-0.14.1 universal-pathlib-0.2.2 urllib3-2.0.7 werkzeug-2.2.3 wrapt-1.14.1 yarl-1.9.4 zipp-3.18.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "certifi",
                  "cffi",
                  "dateutil",
                  "google",
                  "pkg_resources",
                  "psutil",
                  "pygments",
                  "setuptools",
                  "six"
                ]
              },
              "id": "e1ea526104c948e1824b3e3670144cda"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime\n",
        "\n",
        "# Define your DAG\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2022, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'my_dag',\n",
        "    default_args=default_args,\n",
        "    description='A simple DAG',\n",
        "    schedule='@daily',  # Use schedule instead of schedule_interval\n",
        ")\n",
        "\n",
        "# Define your tasks\n",
        "def my_python_function():\n",
        "    print(\"Running Python function\")\n",
        "\n",
        "task1 = PythonOperator(\n",
        "    task_id='python_task',\n",
        "    python_callable=my_python_function,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "task2 = BashOperator(\n",
        "    task_id='bash_task',\n",
        "    bash_command='echo \"Running bash command\"',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "task1 >> task2  # Define task dependencies\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x1c6mppsnna",
        "outputId": "f201aca6-7117-4261-8a9a-abb876a6cb0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Task(BashOperator): bash_task>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "conn = psycopg2.connect(\n",
        "    dbname='pesto_techi',\n",
        "    user='jebin12',\n",
        "    password='jebin@055',\n",
        "    host='localhost',\n",
        "    port='5432'\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Execute SQL command to create the ad_impressions table\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS ad_impressions (\n",
        "        ad_creative_id INT,\n",
        "        user_id VARCHAR(50),\n",
        "        timestamp TIMESTAMP,\n",
        "        website VARCHAR(100)\n",
        "    );\n",
        "\"\"\")\n",
        "\n",
        "# Execute SQL command to create the click_conversions table\n",
        "cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS click_conversions (\n",
        "        user_id VARCHAR(50),\n",
        "        timestamp TIMESTAMP,\n",
        "        ad_campaign_id INT,\n",
        "        conversion_type VARCHAR(50)\n",
        "    );\n",
        "\"\"\")\n",
        "\n",
        "# Create indexes for faster retrieval\n",
        "cur.execute(\"CREATE INDEX ON ad_impressions(ad_creative_id);\")\n",
        "cur.execute(\"CREATE INDEX ON click_conversions(ad_campaign_id);\")\n",
        "\n",
        "# Commit the transaction\n",
        "conn.commit()\n",
        "\n",
        "# Close the cursor and connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "oLXIbtDCty2z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "conn = psycopg2.connect(\n",
        "    dbname='pesto_techi',\n",
        "    user='jebin12',\n",
        "    password='jebin@055',\n",
        "    host='localhost',\n",
        "    port='5432'\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Clear existing data from the click_conversions table\n",
        "cur.execute(\"DELETE FROM click_conversions;\")\n",
        "\n",
        "# Commit the transaction\n",
        "conn.commit()\n",
        "\n",
        "# Execute SQL command to insert data into the click_conversions table if it doesn't exist\n",
        "cur.execute(\"\"\"\n",
        "    INSERT INTO click_conversions (user_id, timestamp, ad_campaign_id, conversion_type)\n",
        "    SELECT * FROM (VALUES\n",
        "        ('101', '2024-05-02T10:05:00'::timestamp, 1, 'sign-up'),\n",
        "        ('102', '2024-05-02T11:15:00'::timestamp, 2, 'purchase')\n",
        "    ) AS data (user_id, timestamp, ad_campaign_id, conversion_type)\n",
        "    WHERE NOT EXISTS (\n",
        "        SELECT 1 FROM click_conversions WHERE user_id = data.user_id AND timestamp = data.timestamp::timestamp\n",
        "    );\n",
        "\"\"\")\n",
        "\n",
        "# Commit the transaction\n",
        "conn.commit()\n",
        "\n",
        "# Execute SQL query to fetch data from the click_conversions table\n",
        "cur.execute(\"SELECT * FROM click_conversions;\")\n",
        "rows = cur.fetchall()\n",
        "\n",
        "# Print fetched data from the click_conversions table\n",
        "print(\"Data from click_conversions table:\")\n",
        "for row in rows:\n",
        "    print(row)\n",
        "\n",
        "# Close the cursor and connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WskB5ACj1-Oz",
        "outputId": "95271cd8-e445-4fb4-ebff-1b8d40450b71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data from click_conversions table:\n",
            "('101', datetime.datetime(2024, 5, 2, 10, 5), 1, 'sign-up')\n",
            "('102', datetime.datetime(2024, 5, 2, 11, 15), 2, 'purchase')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "conn = psycopg2.connect(\n",
        "    dbname='pesto_techi',\n",
        "    user='jebin12',\n",
        "    password='jebin@055',\n",
        "    host='localhost',\n",
        "    port='5432'\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Define SQL commands to create tables for ad impressions and clicks/conversions\n",
        "create_ad_impressions_table = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS ad_impressions (\n",
        "        ad_creative_id INT,\n",
        "        user_id VARCHAR(50),\n",
        "        impression_timestamp TIMESTAMP,\n",
        "        website VARCHAR(100)\n",
        "    );\n",
        "\"\"\"\n",
        "\n",
        "create_clicks_conversions_table = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS clicks_conversions (\n",
        "        user_id VARCHAR(50),\n",
        "        click_conversion_timestamp TIMESTAMP,\n",
        "        ad_campaign_id INT,\n",
        "        conversion_type VARCHAR(50)\n",
        "    );\n",
        "\"\"\"\n",
        "\n",
        "# Execute SQL commands to create tables\n",
        "cur.execute(create_ad_impressions_table)\n",
        "cur.execute(create_clicks_conversions_table)\n",
        "\n",
        "# Commit the transaction\n",
        "conn.commit()\n",
        "\n",
        "# Close the cursor and connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "hEawNqC-2Th2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to the PostgreSQL database\n",
        "conn = psycopg2.connect(\n",
        "    dbname='pesto_techi',\n",
        "    user='jebin12',\n",
        "    password='jebin@055',\n",
        "    host='localhost',\n",
        "    port='5432'\n",
        ")\n",
        "\n",
        "# Create a cursor object\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Define SQL commands to create indexes on ad_impressions and clicks_conversions tables\n",
        "create_ad_impressions_index = \"\"\"\n",
        "    CREATE INDEX IF NOT EXISTS ad_impressions_user_id_idx ON ad_impressions(user_id);\n",
        "\"\"\"\n",
        "\n",
        "create_clicks_conversions_index = \"\"\"\n",
        "    CREATE INDEX IF NOT EXISTS clicks_conversions_user_id_idx ON clicks_conversions(user_id);\n",
        "\"\"\"\n",
        "\n",
        "# Execute SQL commands to create indexes\n",
        "cur.execute(create_ad_impressions_index)\n",
        "cur.execute(create_clicks_conversions_index)\n",
        "\n",
        "# Commit the transaction\n",
        "conn.commit()\n",
        "\n",
        "# Close the cursor and connection\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "5G5mk0883XPm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# Define a custom logger\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create a file handler and set its level to DEBUG\n",
        "file_handler = logging.FileHandler('data_processing.log')\n",
        "file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create a console handler and set its level to ERROR\n",
        "console_handler = logging.StreamHandler(sys.stdout)\n",
        "console_handler.setLevel(logging.ERROR)\n",
        "\n",
        "# Create a formatter and set it for both handlers\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler.setFormatter(formatter)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handlers to the logger\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "\n",
        "# Error Alerting\n",
        "def send_slack_notification(message):\n",
        "    # Placeholder for sending Slack notifications\n",
        "    logger.error(f\"Error occurred: {message}\")\n",
        "    # Add code to send Slack notification here\n",
        "\n",
        "\n",
        "# Data Quality Checks\n",
        "def perform_data_quality_checks(data):\n",
        "    # Placeholder for data quality checks\n",
        "    for item in data:\n",
        "        if not item.get('user_id'):\n",
        "            send_slack_notification(\"Missing user_id in data\")\n",
        "            logger.error(\"Missing user_id in data\")\n",
        "        if not item.get('timestamp'):\n",
        "            send_slack_notification(\"Missing timestamp in data\")\n",
        "            logger.error(\"Missing timestamp in data\")\n",
        "        # Add more data quality checks as needed\n",
        "\n",
        "\n",
        "# Example of using the logger and error alerting in the data processing pipeline\n",
        "def process_data(data):\n",
        "    try:\n",
        "        # Perform data processing operations\n",
        "        # Example: Enrich data\n",
        "        enriched_data = enrich_data(data)\n",
        "\n",
        "        # Example: Validate data\n",
        "        validated_data = validate_data(enriched_data)\n",
        "\n",
        "        # Example: Perform data quality checks\n",
        "        perform_data_quality_checks(validated_data)\n",
        "\n",
        "        # Example: Store processed data in database\n",
        "        store_data(validated_data)\n",
        "    except Exception as e:\n",
        "        # Handle exceptions and send error alert\n",
        "        send_slack_notification(f\"An error occurred during data processing: {str(e)}\")\n",
        "        logger.exception(\"An error occurred during data processing\", exc_info=True)\n",
        "\n",
        "\n",
        "# Example function to simulate data processing operations\n",
        "def enrich_data(data):\n",
        "    # Placeholder for data enrichment logic\n",
        "    return data\n",
        "\n",
        "\n",
        "# Example function to simulate data validation\n",
        "def validate_data(data):\n",
        "    # Placeholder for data validation logic\n",
        "    return data\n",
        "\n",
        "\n",
        "# Example function to store processed data in database\n",
        "def store_data(data):\n",
        "    # Placeholder for storing data in database\n",
        "    pass\n",
        "\n",
        "\n",
        "# Example usage of the data processing pipeline\n",
        "sample_data = [\n",
        "    {\"user_id\": \"101\", \"timestamp\": \"2024-05-02T10:00:00\", \"ad_campaign_id\": 1},\n",
        "    {\"user_id\": \"\", \"timestamp\": \"2024-05-02T11:00:00\", \"ad_campaign_id\": 2},\n",
        "    {\"user_id\": \"103\", \"timestamp\": \"\", \"ad_campaign_id\": 3}\n",
        "]\n",
        "\n",
        "process_data(sample_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FzrEwFA3qZd",
        "outputId": "93435c25-a864-4464-cbb1-98e9840d7c61"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-02 19:55:50,684 - __main__ - ERROR - Error occurred: Missing user_id in data\n",
            "[\u001b[34m2024-05-02T19:55:50.684+0000\u001b[0m] {\u001b[34m<ipython-input-12-255a1aa61a42>:\u001b[0m29} ERROR\u001b[0m - Error occurred: Missing user_id in data\u001b[0m\n",
            "2024-05-02 19:55:50,690 - __main__ - ERROR - Missing user_id in data\n",
            "[\u001b[34m2024-05-02T19:55:50.690+0000\u001b[0m] {\u001b[34m<ipython-input-12-255a1aa61a42>:\u001b[0m39} ERROR\u001b[0m - Missing user_id in data\u001b[0m\n",
            "2024-05-02 19:55:50,694 - __main__ - ERROR - Error occurred: Missing timestamp in data\n",
            "[\u001b[34m2024-05-02T19:55:50.694+0000\u001b[0m] {\u001b[34m<ipython-input-12-255a1aa61a42>:\u001b[0m29} ERROR\u001b[0m - Error occurred: Missing timestamp in data\u001b[0m\n",
            "2024-05-02 19:55:50,699 - __main__ - ERROR - Missing timestamp in data\n",
            "[\u001b[34m2024-05-02T19:55:50.699+0000\u001b[0m] {\u001b[34m<ipython-input-12-255a1aa61a42>:\u001b[0m42} ERROR\u001b[0m - Missing timestamp in data\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJIJ_R3d4J9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}